"""
Isolation Forestë¥¼ ì‚¬ìš©í•œ ì´ìƒ ë°ì´í„° ì›ì¸ ë¶„ì„ ë°ëª¨

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ Isolation Forest ëª¨ë¸ë¡œ:
1. ì´ìƒ ë°ì´í„° ê°ì§€
2. ì–´ë–¤ ì„¼ì„œ/íŠ¹ì§•ì´ ì´ìƒì„ ì¼ìœ¼ì¼°ëŠ”ì§€ ë¶„ì„
3. êµ¬ì²´ì ì¸ ì›ì¸ íŒŒì•…
"""

import mlflow
import mlflow.sklearn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Windows í™˜ê²½ì—ì„œ UTF-8 ì¸ì½”ë”© ì„¤ì •
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

print("="*80)
print("ğŸ” Isolation Forest ì´ìƒ ë°ì´í„° ì›ì¸ ë¶„ì„ ì‹œìŠ¤í…œ")
print("="*80)

# 1. MLflowì—ì„œ í•™ìŠµëœ Isolation Forest ëª¨ë¸ ë¡œë“œ
print("\nğŸ“¦ Step 1: MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
run_id = "c66eb2b2ae5e4057b839264080ce24e5"  # Isolation Forest Run ID

try:
    model = mlflow.sklearn.load_model(f'runs:/{run_id}/model')
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Run ID: {run_id})")
except:
    print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. MLflow UIì—ì„œ Run IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    print("   http://localhost:5000 ì—ì„œ Isolation Forestì˜ Run IDë¥¼ ë³µì‚¬í•˜ì„¸ìš”.")
    exit(1)

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
print("\nğŸ“‚ Step 2: ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_parquet('preprocessed_shaft_data.parquet')
feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if 'timestamp' in feature_cols:
    feature_cols.remove('timestamp')

print(f"   ë°ì´í„° shape: {df.shape}")
print(f"   íŠ¹ì§• ê°œìˆ˜: {len(feature_cols)}")
print(f"   íŠ¹ì§• ëª©ë¡: {feature_cols[:5]}... (ì´ {len(feature_cols)}ê°œ)")

# ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ë²•)
scaler = StandardScaler()
X = scaler.fit_transform(df[feature_cols])

# 3. ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì´ìƒ ì˜ˆì¸¡
print("\nğŸ” Step 3: ì´ìƒ ë°ì´í„° íƒì§€ ì¤‘...")
predictions = model.predict(X)
anomaly_scores = model.score_samples(X)  # ì´ìƒ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)

n_anomalies = (predictions == -1).sum()
anomaly_rate = (n_anomalies / len(predictions)) * 100

print(f"   ì „ì²´ ë°ì´í„°: {len(predictions):,}ê°œ")
print(f"   ì´ìƒ ë°ì´í„°: {n_anomalies:,}ê°œ ({anomaly_rate:.2f}%)")
print(f"   ì •ìƒ ë°ì´í„°: {(predictions == 1).sum():,}ê°œ")

# 4. ì´ìƒ ë°ì´í„° ìƒ˜í”Œ ì„ íƒ ë° ë¶„ì„
print("\nğŸ¯ Step 4: ì´ìƒ ë°ì´í„° ìƒìœ„ 5ê°œ ë¶„ì„")
print("-"*80)

# ì´ìƒ ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ (ê°€ì¥ ì´ìƒí•œ) 5ê°œ ìƒ˜í”Œ ì„ íƒ
anomaly_indices = np.where(predictions == -1)[0]
top_anomalies_idx = anomaly_indices[np.argsort(anomaly_scores[anomaly_indices])[:5]]

for i, idx in enumerate(top_anomalies_idx, 1):
    print(f"\nã€ì´ìƒ ë°ì´í„° #{i}ã€‘")
    print(f"   ì¸ë±ìŠ¤: {idx}")
    print(f"   ì´ìƒ ì ìˆ˜: {anomaly_scores[idx]:.6f} (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)")

    # í•´ë‹¹ ìƒ˜í”Œì˜ ì›ë³¸ ê°’
    sample = df.iloc[idx][feature_cols]
    sample_scaled = X[idx]

    # ê° íŠ¹ì§•ì˜ ê¸°ì—¬ë„ ë¶„ì„
    # ë°©ë²•: ì •ìƒ ë°ì´í„°ì˜ í‰ê· /í‘œì¤€í¸ì°¨ì™€ ë¹„êµ
    normal_indices = np.where(predictions == 1)[0]
    normal_mean = X[normal_indices].mean(axis=0)
    normal_std = X[normal_indices].std(axis=0)

    # Z-score ê³„ì‚° (ì •ê·œí™”ëœ ê°’ì—ì„œ)
    z_scores = np.abs((sample_scaled - normal_mean) / (normal_std + 1e-8))

    # ê°€ì¥ ì´ìƒí•œ ìƒìœ„ 5ê°œ íŠ¹ì§•
    top_features_idx = np.argsort(z_scores)[-5:][::-1]

    print(f"\n   ğŸš¨ ì´ìƒ ì›ì¸ (ìƒìœ„ 5ê°œ ì„¼ì„œ):")
    for rank, feat_idx in enumerate(top_features_idx, 1):
        feat_name = feature_cols[feat_idx]
        z_score = z_scores[feat_idx]
        actual_value = sample[feat_name]
        normal_avg = df.iloc[normal_indices][feat_name].mean()

        deviation = ((actual_value - normal_avg) / normal_avg * 100) if normal_avg != 0 else 0

        print(f"      {rank}. {feat_name:30s}: Z-score={z_score:.2f}")
        print(f"         ì‹¤ì œê°’: {actual_value:10.4f} | ì •ìƒí‰ê· : {normal_avg:10.4f} | í¸ì°¨: {deviation:+.1f}%")

# 5. Feature Importance ë¶„ì„ (ì „ì²´ ëª¨ë¸ ê¸°ì¤€)
print("\n" + "="*80)
print("ğŸ“Š Step 5: ì „ì²´ ëª¨ë¸ì˜ Feature Importance ë¶„ì„")
print("-"*80)

# Isolation Forestì—ì„œ ì§ì ‘ feature importanceë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
# ì´ìƒ ë°ì´í„°ì—ì„œ ê° íŠ¹ì§•ì˜ í‰ê·  ê¸°ì—¬ë„ë¥¼ ê³„ì‚°
if len(anomaly_indices) > 0:
    # ì´ìƒ ë°ì´í„°ì˜ í‰ê·  Z-score
    anomaly_samples = X[anomaly_indices]
    normal_mean = X[predictions == 1].mean(axis=0)
    normal_std = X[predictions == 1].std(axis=0)

    avg_z_scores = np.abs((anomaly_samples - normal_mean) / (normal_std + 1e-8)).mean(axis=0)

    # ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•
    top_10_features = np.argsort(avg_z_scores)[-10:][::-1]

    print("\nğŸ” ì´ìƒ íƒì§€ì— ê°€ì¥ ì¤‘ìš”í•œ ì„¼ì„œ Top 10:")
    for rank, feat_idx in enumerate(top_10_features, 1):
        feat_name = feature_cols[feat_idx]
        importance = avg_z_scores[feat_idx]
        print(f"   {rank:2d}. {feat_name:35s}: {importance:.4f}")

# 6. ì‹œê°í™”
print("\nğŸ“ˆ Step 6: ì‹œê°í™” ìƒì„± ì¤‘...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Isolation Forest ì´ìƒ íƒì§€ ìƒì„¸ ë¶„ì„', fontsize=16, fontweight='bold')

# 6-1. ì´ìƒ ì ìˆ˜ ë¶„í¬
ax1 = axes[0, 0]
ax1.hist(anomaly_scores[predictions == 1], bins=50, alpha=0.7, label='Normal', color='green')
ax1.hist(anomaly_scores[predictions == -1], bins=50, alpha=0.7, label='Anomaly', color='red')
ax1.set_xlabel('Anomaly Score', fontsize=12)
ax1.set_ylabel('Frequency', fontsize=12)
ax1.set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 6-2. Feature Importance (Top 10)
ax2 = axes[0, 1]
top_10_names = [feature_cols[i] for i in top_10_features]
top_10_values = [avg_z_scores[i] for i in top_10_features]
colors = plt.cm.Reds(np.linspace(0.4, 0.9, 10))
ax2.barh(range(10), top_10_values[::-1], color=colors[::-1])
ax2.set_yticks(range(10))
ax2.set_yticklabels(top_10_names[::-1], fontsize=10)
ax2.set_xlabel('Average Z-Score (Anomaly Contribution)', fontsize=12)
ax2.set_title('Top 10 Important Features', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')

# 6-3. ì •ìƒ vs ì´ìƒ ë°ì´í„° ë¹„ìœ¨
ax3 = axes[1, 0]
labels = ['Normal', 'Anomaly']
sizes = [(predictions == 1).sum(), (predictions == -1).sum()]
colors_pie = ['#90EE90', '#FF6B6B']
explode = (0, 0.1)
ax3.pie(sizes, explode=explode, labels=labels, colors=colors_pie, autopct='%1.1f%%',
        startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
ax3.set_title('Normal vs Anomaly Distribution', fontsize=14, fontweight='bold')

# 6-4. ìƒìœ„ 3ê°œ ì´ìƒ ìƒ˜í”Œì˜ Feature Profile
ax4 = axes[1, 1]
for i, idx in enumerate(top_anomalies_idx[:3]):
    sample = df.iloc[idx][feature_cols]
    sample_scaled = X[idx]
    z_scores_sample = np.abs((sample_scaled - normal_mean) / (normal_std + 1e-8))
    top_5_for_sample = np.argsort(z_scores_sample)[-5:][::-1]

    feat_names = [feature_cols[j][:15] for j in top_5_for_sample]
    feat_values = [z_scores_sample[j] for j in top_5_for_sample]

    x_pos = np.arange(5)
    ax4.plot(x_pos, feat_values, marker='o', linewidth=2, markersize=8, label=f'Anomaly #{i+1}')

ax4.set_xticks(range(5))
ax4.set_xticklabels(['Top 1', 'Top 2', 'Top 3', 'Top 4', 'Top 5'], fontsize=10)
ax4.set_ylabel('Z-Score', fontsize=12)
ax4.set_title('Top 3 Anomalies - Feature Contribution Profile', fontsize=14, fontweight='bold')
ax4.legend(fontsize=10)
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('isolation_forest_detailed_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: isolation_forest_detailed_analysis.png")

# 7. ì‹¤ë¬´ í™œìš© ì˜ˆì œ
print("\n" + "="*80)
print("ğŸ’¼ Step 7: ì‹¤ë¬´ í™œìš© ì˜ˆì œ - ìƒˆë¡œìš´ ë°ì´í„° ë¶„ì„")
print("-"*80)

# ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ë‹¤ê³  ê°€ì •
print("\nì‹œë‚˜ë¦¬ì˜¤: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ ìƒˆë¡œìš´ ì„¼ì„œ ë°ì´í„°ê°€ ë“¤ì–´ì˜´")

# ì„ì˜ì˜ ìƒ˜í”Œ ì„ íƒ (ì‹¤ì œë¡œëŠ” ì‹¤ì‹œê°„ ë°ì´í„°)
new_data_idx = np.random.choice(len(df), 1)[0]
new_data = df.iloc[new_data_idx][feature_cols].values.reshape(1, -1)
new_data_scaled = scaler.transform(new_data)

# ì˜ˆì¸¡
prediction = model.predict(new_data_scaled)[0]
score = model.score_samples(new_data_scaled)[0]

print(f"\nì…ë ¥ ë°ì´í„° ì¸ë±ìŠ¤: {new_data_idx}")
print(f"ì˜ˆì¸¡ ê²°ê³¼: {'ğŸš¨ ì´ìƒ!' if prediction == -1 else 'âœ… ì •ìƒ'}")
print(f"ì´ìƒ ì ìˆ˜: {score:.6f}")

if prediction == -1:
    print(f"\nâš ï¸ ì´ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ì–´ë–¤ ì„¼ì„œê°€ ë¬¸ì œì¸ì§€ ë¶„ì„
    sample_scaled = new_data_scaled[0]
    z_scores = np.abs((sample_scaled - normal_mean) / (normal_std + 1e-8))
    top_problems = np.argsort(z_scores)[-5:][::-1]

    print(f"\nğŸ”§ ì ê²€ì´ í•„ìš”í•œ ì„¼ì„œ (ìš°ì„ ìˆœìœ„ ìˆœ):")
    for rank, feat_idx in enumerate(top_problems, 1):
        feat_name = feature_cols[feat_idx]
        z_score = z_scores[feat_idx]
        actual = new_data[0, feat_idx]
        normal_avg = df.iloc[normal_indices][feat_name].mean()

        print(f"   {rank}. {feat_name:30s}")
        print(f"      â†’ í˜„ì¬ê°’: {actual:10.4f} | ì •ìƒí‰ê· : {normal_avg:10.4f}")
        print(f"      â†’ Z-score: {z_score:.2f} (ì´ìƒë„)")

    print(f"\nğŸ’¡ ì¡°ì¹˜ ê¶Œê³ :")
    print(f"   1. {feature_cols[top_problems[0]]} ì„¼ì„œ ìš°ì„  ì ê²€")
    print(f"   2. ìƒìœ„ 3ê°œ ì„¼ì„œì˜ ìƒê´€ê´€ê³„ ë¶„ì„")
    print(f"   3. ì •ë¹„íŒ€ì— ì•Œë¦¼ ë°œì†¡")
else:
    print(f"\nâœ… ì •ìƒ ë°ì´í„°ì…ë‹ˆë‹¤. ê³„ì† ëª¨ë‹ˆí„°ë§...")

# 8. API í™œìš© ì˜ˆì œ ì½”ë“œ
print("\n" + "="*80)
print("ğŸ”Œ Step 8: API ì„œë²„ êµ¬ì¶• ì˜ˆì œ ì½”ë“œ")
print("-"*80)

api_example = '''
# FastAPIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ API ì˜ˆì œ

from fastapi import FastAPI
import mlflow.sklearn
import numpy as np
import pandas as pd

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
model = mlflow.sklearn.load_model('runs:/c66eb2b2ae5e4057b839264080ce24e5/model')
scaler = StandardScaler()  # ì‚¬ì „ í•™ìŠµëœ scaler ë¡œë“œ í•„ìš”

@app.post("/predict")
async def predict_anomaly(data: dict):
    """
    ì…ë ¥: {"sensor_values": [v1, v2, v3, ..., v39]}
    ì¶œë ¥: {"is_anomaly": true/false, "score": -0.123, "top_features": [...]}
    """
    # ë°ì´í„° ì „ì²˜ë¦¬
    sensor_values = np.array(data["sensor_values"]).reshape(1, -1)
    sensor_scaled = scaler.transform(sensor_values)

    # ì˜ˆì¸¡
    prediction = model.predict(sensor_scaled)[0]
    score = model.score_samples(sensor_scaled)[0]

    # ì›ì¸ ë¶„ì„
    if prediction == -1:
        z_scores = calculate_feature_importance(sensor_scaled)
        top_features = get_top_features(z_scores, k=5)
    else:
        top_features = []

    return {
        "is_anomaly": bool(prediction == -1),
        "anomaly_score": float(score),
        "top_problematic_features": top_features,
        "recommended_action": "Inspect sensors" if prediction == -1 else "Continue monitoring"
    }

# ì‹¤í–‰: uvicorn api:app --reload
'''

print(api_example)

print("\n" + "="*80)
print("âœ… ë¶„ì„ ì™„ë£Œ!")
print("="*80)
print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print("   - isolation_forest_detailed_analysis.png")
print("\nğŸŒ MLflow UIì—ì„œ ë” ë§ì€ ì •ë³´ í™•ì¸:")
print("   http://localhost:5000")
print("\nğŸ’¡ í•µì‹¬ ìš”ì•½:")
print("   1. Isolation ForestëŠ” ì´ìƒì„ ê°ì§€í•˜ê³ ")
print("   2. ì–´ë–¤ ì„¼ì„œê°€ ë¬¸ì œì¸ì§€ ì •í™•íˆ ì•Œë ¤ì¤ë‹ˆë‹¤")
print("   3. Z-scoreë¡œ ê° ì„¼ì„œì˜ ê¸°ì—¬ë„ë¥¼ ì •ëŸ‰í™”í•©ë‹ˆë‹¤")
print("   4. ì‹¤ì‹œê°„ APIë¡œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")
print("="*80)
