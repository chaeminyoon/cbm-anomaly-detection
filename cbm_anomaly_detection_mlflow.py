"""
CBM ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ - MLflow í†µí•© ë²„ì „
ì¶”ì§„ì¶•ê³„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ ë° ì¶”ì 

MLflowë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜, íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤.
"""

# Windows í™˜ê²½ì—ì„œ UTF-8 ì¸ì½”ë”© ì„¤ì •
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping
import pickle
import mlflow
import mlflow.keras
import mlflow.sklearn
from mlflow.tracking import MlflowClient
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False


class AnomalyDetectionMLflow:
    """MLflowë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ"""

    def __init__(self, data_path='preprocessed_shaft_data.parquet', experiment_name='CBM_Shaft_Anomaly_Detection'):
        """
        ì´ˆê¸°í™”

        Args:
            data_path: ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²½ë¡œ
            experiment_name: MLflow ì‹¤í—˜ ì´ë¦„
        """
        self.data_path = data_path
        self.experiment_name = experiment_name

        # MLflow ì‹¤í—˜ ì„¤ì •
        mlflow.set_experiment(experiment_name)
        print(f"âœ… MLflow ì‹¤í—˜ ì„¤ì •: {experiment_name}")

        # ë°ì´í„° ë¡œë“œ
        self.load_data()

    def load_data(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {self.data_path}")
        self.df = pd.read_parquet(self.data_path)
        print(f"   ë°ì´í„° shape: {self.df.shape}")

        # íŠ¹ì§• ì»¬ëŸ¼ (ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ)
        self.feature_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        if 'timestamp' in self.feature_cols:
            self.feature_cols.remove('timestamp')

        print(f"   íŠ¹ì§• ì»¬ëŸ¼ ìˆ˜: {len(self.feature_cols)}")

    def prepare_data(self, test_size=0.2, random_state=42):
        """ë°ì´í„° ì¤€ë¹„ ë° ì •ê·œí™”"""
        print(f"\nğŸ”§ ë°ì´í„° ì¤€ë¹„ ì¤‘ (test_size={test_size})")

        # ê²°ì¸¡ì¹˜ ì œê±°
        self.df = self.df.dropna(subset=self.feature_cols)

        # Train/Test ë¶„í• 
        train_df, test_df = train_test_split(
            self.df,
            test_size=test_size,
            random_state=random_state
        )

        # ì •ê·œí™”
        self.scaler = StandardScaler()
        self.X_train = self.scaler.fit_transform(train_df[self.feature_cols])
        self.X_test = self.scaler.transform(test_df[self.feature_cols])

        print(f"   Train shape: {self.X_train.shape}")
        print(f"   Test shape: {self.X_test.shape}")

        return self.X_train, self.X_test

    def train_autoencoder(self, encoding_dim=32, epochs=50, batch_size=256):
        """
        Autoencoder ëª¨ë¸ í•™ìŠµ (MLflow ì¶”ì )

        Args:
            encoding_dim: ì¸ì½”ë”© ì°¨ì›
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        print(f"\nğŸš€ Autoencoder ëª¨ë¸ í•™ìŠµ ì‹œì‘")

        with mlflow.start_run(run_name="Autoencoder") as run:
            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            params = {
                "model_type": "Autoencoder",
                "encoding_dim": encoding_dim,
                "epochs": epochs,
                "batch_size": batch_size,
                "input_dim": self.X_train.shape[1],
                "optimizer": "adam",
                "loss": "mse"
            }
            mlflow.log_params(params)

            # ëª¨ë¸ êµ¬ì„±
            input_dim = self.X_train.shape[1]
            input_layer = Input(shape=(input_dim,))

            # Encoder
            encoded = Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = Dense(encoding_dim, activation='relu')(encoded)

            # Decoder
            decoded = Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = Dense(input_dim, activation='linear')(decoded)

            # ëª¨ë¸ ìƒì„±
            autoencoder = Model(inputs=input_layer, outputs=decoded)
            autoencoder.compile(optimizer='adam', loss='mse')

            # Early Stopping
            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            )

            # í•™ìŠµ
            history = autoencoder.fit(
                self.X_train, self.X_train,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=0.1,
                callbacks=[early_stop],
                verbose=1
            )

            # ë©”íŠ¸ë¦­ ë¡œê¹…
            final_train_loss = history.history['loss'][-1]
            final_val_loss = history.history['val_loss'][-1]

            mlflow.log_metric("final_train_loss", final_train_loss)
            mlflow.log_metric("final_val_loss", final_val_loss)
            mlflow.log_metric("epochs_trained", len(history.history['loss']))

            # ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°
            train_pred = autoencoder.predict(self.X_train, verbose=0)
            test_pred = autoencoder.predict(self.X_test, verbose=0)

            train_mse = np.mean(np.power(self.X_train - train_pred, 2), axis=1)
            test_mse = np.mean(np.power(self.X_test - test_pred, 2), axis=1)

            # Threshold ê³„ì‚° (95 percentile)
            threshold = np.percentile(train_mse, 95)
            mlflow.log_metric("threshold", threshold)

            # ì´ìƒì¹˜ íƒì§€
            train_anomalies = train_mse > threshold
            test_anomalies = test_mse > threshold

            train_anomaly_rate = np.mean(train_anomalies) * 100
            test_anomaly_rate = np.mean(test_anomalies) * 100

            mlflow.log_metric("train_anomaly_rate", train_anomaly_rate)
            mlflow.log_metric("test_anomaly_rate", test_anomaly_rate)

            print(f"   âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"   - Threshold: {threshold:.6f}")
            print(f"   - Train ì´ìƒ ë¹„ìœ¨: {train_anomaly_rate:.2f}%")
            print(f"   - Test ì´ìƒ ë¹„ìœ¨: {test_anomaly_rate:.2f}%")

            # ì‹œê°í™”
            self._plot_autoencoder_results(
                history, train_mse, test_mse, threshold,
                train_anomalies, test_anomalies
            )

            # ëª¨ë¸ ì €ì¥
            mlflow.keras.log_model(autoencoder, "model")

            # Threshold ì €ì¥
            np.save('autoencoder_threshold.npy', threshold)
            mlflow.log_artifact('autoencoder_threshold.npy')

            # ì‹œê°í™” ì €ì¥
            mlflow.log_artifact('autoencoder_training_history.png')
            mlflow.log_artifact('autoencoder_anomaly_visualization.png')

            # Run ID ì €ì¥
            self.autoencoder_run_id = run.info.run_id
            print(f"   ğŸ“Š MLflow Run ID: {self.autoencoder_run_id}")

            return autoencoder, threshold

    def train_isolation_forest(self, contamination=0.05, n_estimators=100):
        """
        Isolation Forest ëª¨ë¸ í•™ìŠµ (MLflow ì¶”ì )

        Args:
            contamination: ì´ìƒì¹˜ ë¹„ìœ¨
            n_estimators: íŠ¸ë¦¬ ê°œìˆ˜
        """
        print(f"\nğŸŒ² Isolation Forest ëª¨ë¸ í•™ìŠµ ì‹œì‘")

        with mlflow.start_run(run_name="IsolationForest") as run:
            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            params = {
                "model_type": "IsolationForest",
                "contamination": contamination,
                "n_estimators": n_estimators,
                "max_samples": "auto",
                "random_state": 42
            }
            mlflow.log_params(params)

            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            iso_forest = IsolationForest(
                contamination=contamination,
                n_estimators=n_estimators,
                max_samples='auto',
                random_state=42,
                n_jobs=-1
            )

            iso_forest.fit(self.X_train)

            # ì˜ˆì¸¡
            train_pred = iso_forest.predict(self.X_train)
            test_pred = iso_forest.predict(self.X_test)

            # ì´ìƒ ì ìˆ˜
            train_scores = iso_forest.score_samples(self.X_train)
            test_scores = iso_forest.score_samples(self.X_test)

            # ì´ìƒì¹˜ íƒì§€ (-1: ì´ìƒ, 1: ì •ìƒ)
            train_anomalies = train_pred == -1
            test_anomalies = test_pred == -1

            train_anomaly_rate = np.mean(train_anomalies) * 100
            test_anomaly_rate = np.mean(test_anomalies) * 100

            # ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_metric("train_anomaly_rate", train_anomaly_rate)
            mlflow.log_metric("test_anomaly_rate", test_anomaly_rate)
            mlflow.log_metric("mean_train_score", np.mean(train_scores))
            mlflow.log_metric("mean_test_score", np.mean(test_scores))

            print(f"   âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"   - Train ì´ìƒ ë¹„ìœ¨: {train_anomaly_rate:.2f}%")
            print(f"   - Test ì´ìƒ ë¹„ìœ¨: {test_anomaly_rate:.2f}%")

            # ì‹œê°í™”
            self._plot_isolation_forest_results(
                train_scores, test_scores,
                train_anomalies, test_anomalies
            )

            # ëª¨ë¸ ì €ì¥
            mlflow.sklearn.log_model(iso_forest, "model")

            # Pickle ì €ì¥
            with open('isolation_forest_model.pkl', 'wb') as f:
                pickle.dump(iso_forest, f)
            mlflow.log_artifact('isolation_forest_model.pkl')

            # ì‹œê°í™” ì €ì¥
            mlflow.log_artifact('isolation_forest_anomaly_visualization.png')

            # Run ID ì €ì¥
            self.isolation_forest_run_id = run.info.run_id
            print(f"   ğŸ“Š MLflow Run ID: {self.isolation_forest_run_id}")

            return iso_forest

    def train_lstm_autoencoder(self, encoding_dim=32, epochs=50, batch_size=256, timesteps=10):
        """
        LSTM Autoencoder ëª¨ë¸ í•™ìŠµ (MLflow ì¶”ì )

        Args:
            encoding_dim: ì¸ì½”ë”© ì°¨ì›
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            timesteps: ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
        """
        print(f"\nğŸ”„ LSTM Autoencoder ëª¨ë¸ í•™ìŠµ ì‹œì‘")

        with mlflow.start_run(run_name="LSTM_Autoencoder") as run:
            # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
            X_train_lstm = self._create_sequences(self.X_train, timesteps)
            X_test_lstm = self._create_sequences(self.X_test, timesteps)

            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            params = {
                "model_type": "LSTM_Autoencoder",
                "encoding_dim": encoding_dim,
                "epochs": epochs,
                "batch_size": batch_size,
                "timesteps": timesteps,
                "input_dim": X_train_lstm.shape[2],
                "optimizer": "adam",
                "loss": "mse"
            }
            mlflow.log_params(params)

            # ëª¨ë¸ êµ¬ì„±
            input_dim = X_train_lstm.shape[2]

            # Encoder
            inputs = Input(shape=(timesteps, input_dim))
            encoded = LSTM(encoding_dim, activation='relu', return_sequences=False)(inputs)

            # Decoder
            decoded = RepeatVector(timesteps)(encoded)
            decoded = LSTM(encoding_dim, activation='relu', return_sequences=True)(decoded)
            decoded = TimeDistributed(Dense(input_dim))(decoded)

            # ëª¨ë¸ ìƒì„±
            lstm_autoencoder = Model(inputs=inputs, outputs=decoded)
            lstm_autoencoder.compile(optimizer='adam', loss='mse')

            # Early Stopping
            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            )

            # í•™ìŠµ
            history = lstm_autoencoder.fit(
                X_train_lstm, X_train_lstm,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=0.1,
                callbacks=[early_stop],
                verbose=1
            )

            # ë©”íŠ¸ë¦­ ë¡œê¹…
            final_train_loss = history.history['loss'][-1]
            final_val_loss = history.history['val_loss'][-1]

            mlflow.log_metric("final_train_loss", final_train_loss)
            mlflow.log_metric("final_val_loss", final_val_loss)
            mlflow.log_metric("epochs_trained", len(history.history['loss']))

            # ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°
            train_pred = lstm_autoencoder.predict(X_train_lstm, verbose=0)
            test_pred = lstm_autoencoder.predict(X_test_lstm, verbose=0)

            train_mse = np.mean(np.power(X_train_lstm - train_pred, 2), axis=(1, 2))
            test_mse = np.mean(np.power(X_test_lstm - test_pred, 2), axis=(1, 2))

            # Threshold ê³„ì‚°
            threshold = np.percentile(train_mse, 95)
            mlflow.log_metric("threshold", threshold)

            # ì´ìƒì¹˜ íƒì§€
            train_anomalies = train_mse > threshold
            test_anomalies = test_mse > threshold

            train_anomaly_rate = np.mean(train_anomalies) * 100
            test_anomaly_rate = np.mean(test_anomalies) * 100

            mlflow.log_metric("train_anomaly_rate", train_anomaly_rate)
            mlflow.log_metric("test_anomaly_rate", test_anomaly_rate)

            print(f"   âœ… í•™ìŠµ ì™„ë£Œ!")
            print(f"   - Threshold: {threshold:.6f}")
            print(f"   - Train ì´ìƒ ë¹„ìœ¨: {train_anomaly_rate:.2f}%")
            print(f"   - Test ì´ìƒ ë¹„ìœ¨: {test_anomaly_rate:.2f}%")

            # ì‹œê°í™”
            self._plot_lstm_autoencoder_results(
                history, train_mse, test_mse, threshold,
                train_anomalies, test_anomalies
            )

            # ëª¨ë¸ ì €ì¥
            mlflow.keras.log_model(lstm_autoencoder, "model")

            # Threshold ì €ì¥
            np.save('lstm_autoencoder_threshold.npy', threshold)
            mlflow.log_artifact('lstm_autoencoder_threshold.npy')

            # ì‹œê°í™” ì €ì¥
            mlflow.log_artifact('lstm_autoencoder_anomaly_visualization.png')

            # Run ID ì €ì¥
            self.lstm_autoencoder_run_id = run.info.run_id
            print(f"   ğŸ“Š MLflow Run ID: {self.lstm_autoencoder_run_id}")

            return lstm_autoencoder, threshold

    def _create_sequences(self, data, timesteps):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
        sequences = []
        for i in range(len(data) - timesteps + 1):
            sequences.append(data[i:i+timesteps])
        return np.array(sequences)

    def _plot_autoencoder_results(self, history, train_mse, test_mse, threshold,
                                   train_anomalies, test_anomalies):
        """Autoencoder ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        axes[0, 0].plot(history.history['loss'], label='Train Loss')
        axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Autoencoder í•™ìŠµ íˆìŠ¤í† ë¦¬')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬
        axes[0, 1].hist(train_mse, bins=50, alpha=0.7, label='Train')
        axes[0, 1].hist(test_mse, bins=50, alpha=0.7, label='Test')
        axes[0, 1].axvline(threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
        axes[0, 1].set_xlabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)')
        axes[0, 1].set_ylabel('ë¹ˆë„')
        axes[0, 1].set_title('ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Train ì´ìƒì¹˜ íƒì§€
        axes[1, 0].scatter(range(len(train_mse)), train_mse, c=train_anomalies,
                          cmap='coolwarm', alpha=0.6, s=1)
        axes[1, 0].axhline(threshold, color='r', linestyle='--', label='Threshold')
        axes[1, 0].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[1, 0].set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨')
        axes[1, 0].set_title('Train ì´ìƒì¹˜ íƒì§€')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # Test ì´ìƒì¹˜ íƒì§€
        axes[1, 1].scatter(range(len(test_mse)), test_mse, c=test_anomalies,
                          cmap='coolwarm', alpha=0.6, s=1)
        axes[1, 1].axhline(threshold, color='r', linestyle='--', label='Threshold')
        axes[1, 1].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[1, 1].set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨')
        axes[1, 1].set_title('Test ì´ìƒì¹˜ íƒì§€')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig('autoencoder_anomaly_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()

        # í•™ìŠµ íˆìŠ¤í† ë¦¬ë§Œ ë³„ë„ ì €ì¥
        plt.figure(figsize=(10, 6))
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Autoencoder í•™ìŠµ íˆìŠ¤í† ë¦¬')
        plt.legend()
        plt.grid(True)
        plt.savefig('autoencoder_training_history.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_isolation_forest_results(self, train_scores, test_scores,
                                       train_anomalies, test_anomalies):
        """Isolation Forest ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # ì´ìƒ ì ìˆ˜ ë¶„í¬
        axes[0, 0].hist(train_scores, bins=50, alpha=0.7, label='Train')
        axes[0, 0].hist(test_scores, bins=50, alpha=0.7, label='Test')
        axes[0, 0].set_xlabel('ì´ìƒ ì ìˆ˜')
        axes[0, 0].set_ylabel('ë¹ˆë„')
        axes[0, 0].set_title('Isolation Forest ì´ìƒ ì ìˆ˜ ë¶„í¬')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # Train ì ìˆ˜ ì‹œê³„ì—´
        axes[0, 1].scatter(range(len(train_scores)), train_scores,
                          c=train_anomalies, cmap='coolwarm', alpha=0.6, s=1)
        axes[0, 1].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[0, 1].set_ylabel('ì´ìƒ ì ìˆ˜')
        axes[0, 1].set_title('Train ì´ìƒ ì ìˆ˜')
        axes[0, 1].grid(True)

        # Test ì ìˆ˜ ì‹œê³„ì—´
        axes[1, 0].scatter(range(len(test_scores)), test_scores,
                          c=test_anomalies, cmap='coolwarm', alpha=0.6, s=1)
        axes[1, 0].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[1, 0].set_ylabel('ì´ìƒ ì ìˆ˜')
        axes[1, 0].set_title('Test ì´ìƒ ì ìˆ˜')
        axes[1, 0].grid(True)

        # ì´ìƒì¹˜ ë¹„ìœ¨
        train_rate = np.mean(train_anomalies) * 100
        test_rate = np.mean(test_anomalies) * 100

        axes[1, 1].bar(['Train', 'Test'], [train_rate, test_rate], color=['#1f77b4', '#ff7f0e'])
        axes[1, 1].set_ylabel('ì´ìƒì¹˜ ë¹„ìœ¨ (%)')
        axes[1, 1].set_title('ì´ìƒì¹˜ íƒì§€ ë¹„ìœ¨')
        axes[1, 1].grid(True, axis='y')

        for i, v in enumerate([train_rate, test_rate]):
            axes[1, 1].text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig('isolation_forest_anomaly_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_lstm_autoencoder_results(self, history, train_mse, test_mse, threshold,
                                       train_anomalies, test_anomalies):
        """LSTM Autoencoder ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        axes[0, 0].plot(history.history['loss'], label='Train Loss')
        axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('LSTM Autoencoder í•™ìŠµ íˆìŠ¤í† ë¦¬')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬
        axes[0, 1].hist(train_mse, bins=50, alpha=0.7, label='Train')
        axes[0, 1].hist(test_mse, bins=50, alpha=0.7, label='Test')
        axes[0, 1].axvline(threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
        axes[0, 1].set_xlabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)')
        axes[0, 1].set_ylabel('ë¹ˆë„')
        axes[0, 1].set_title('ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Train ì´ìƒì¹˜
        axes[1, 0].scatter(range(len(train_mse)), train_mse, c=train_anomalies,
                          cmap='coolwarm', alpha=0.6, s=1)
        axes[1, 0].axhline(threshold, color='r', linestyle='--', label='Threshold')
        axes[1, 0].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[1, 0].set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨')
        axes[1, 0].set_title('Train ì´ìƒì¹˜ íƒì§€')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # Test ì´ìƒì¹˜
        axes[1, 1].scatter(range(len(test_mse)), test_mse, c=test_anomalies,
                          cmap='coolwarm', alpha=0.6, s=1)
        axes[1, 1].axhline(threshold, color='r', linestyle='--', label='Threshold')
        axes[1, 1].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')
        axes[1, 1].set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨')
        axes[1, 1].set_title('Test ì´ìƒì¹˜ íƒì§€')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig('lstm_autoencoder_anomaly_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()

    def compare_models(self):
        """MLflowì—ì„œ ëª¨ë¸ ë¹„êµ"""
        print("\nğŸ“Š MLflowì—ì„œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

        client = MlflowClient()
        experiment = client.get_experiment_by_name(self.experiment_name)

        if experiment is None:
            print("   âš ï¸ ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        runs = client.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"]
        )

        print(f"\n{'Model Type':<20} {'Test Anomaly Rate':<20} {'Run ID':<40}")
        print("-" * 80)

        for run in runs:
            model_type = run.data.params.get('model_type', 'Unknown')
            test_anomaly_rate = run.data.metrics.get('test_anomaly_rate', 0)
            run_id = run.info.run_id

            print(f"{model_type:<20} {test_anomaly_rate:<20.2f} {run_id:<40}")

        print("\nğŸ’¡ MLflow UIì—ì„œ ë” ìì„¸í•œ ë¹„êµë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print("   mlflow ui --port 5000")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ”§ CBM ì¶”ì§„ì¶•ê³„ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ - MLflow í†µí•© ë²„ì „")
    print("=" * 80)

    # ì´ˆê¸°í™”
    detector = AnomalyDetectionMLflow(
        data_path='preprocessed_shaft_data.parquet',
        experiment_name='CBM_Shaft_Anomaly_Detection'
    )

    # ë°ì´í„° ì¤€ë¹„
    X_train, X_test = detector.prepare_data(test_size=0.2, random_state=42)

    # 1. Autoencoder í•™ìŠµ
    autoencoder, ae_threshold = detector.train_autoencoder(
        encoding_dim=32,
        epochs=50,
        batch_size=256
    )

    # 2. Isolation Forest í•™ìŠµ
    iso_forest = detector.train_isolation_forest(
        contamination=0.05,
        n_estimators=100
    )

    # 3. LSTM Autoencoder í•™ìŠµ
    lstm_ae, lstm_threshold = detector.train_lstm_autoencoder(
        encoding_dim=32,
        epochs=50,
        batch_size=256,
        timesteps=10
    )

    # ëª¨ë¸ ë¹„êµ
    detector.compare_models()

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° MLflow ì¶”ì  ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ“Š MLflow UI ì‹¤í–‰:")
    print("   mlflow ui --port 5000")
    print("   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ì ‘ì†")
    print("\nğŸ” ëª¨ë¸ ë¡œë“œ ì˜ˆì œ:")
    print(f"   import mlflow")
    print(f"   model = mlflow.keras.load_model('runs:/{detector.autoencoder_run_id}/model')")


if __name__ == "__main__":
    main()
